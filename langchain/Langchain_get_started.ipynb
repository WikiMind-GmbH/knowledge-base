{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Get started\n",
        "This is the Get Started section of\n",
        "https://python.langchain.com/docs/tutorials/\n",
        "\n"
      ],
      "metadata": {
        "id": "B0FzzPiL7F3H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chat models and prompts\n",
        "This contains the Chat models and prompts bullet point of Get started\n",
        "\n",
        "More about how to interact with models and which are supported can be found in this concept guide https://python.langchain.com/docs/concepts/chat_models/\n",
        "\n",
        "(The `Runnable` interface is implemented by a lot of classes, including the different prompt classes. Here is its [concept guide](https://python.langchain.com/docs/concepts/runnables/))"
      ],
      "metadata": {
        "id": "pbExgg5l9P8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "7I02WLwD7Mgz",
        "outputId": "0b19b95a-9dea-453e-d324-0ee4c049a0a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.11)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.11.10)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.24 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.24)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.2)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.3)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.10.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.24->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.24->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.24->langchain) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.24->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-openai"
      ],
      "metadata": {
        "id": "5YJ3RnT59YFo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "014d7845-c8d6-47eb-9839-4d5314a5525e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.3/454.3 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLovybnl9jcl",
        "outputId": "9d5a2ed5-edc1-45bd-8a5d-357ccba76816"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter API key for OpenAI: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Invoke our model with two messages. This is the basic way to use it."
      ],
      "metadata": {
        "id": "ipFDGQCE69Gm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(\"Identify the language or dialect of the userinput\"),\n",
        "    HumanMessage(\"Griaß di\")\n",
        "]\n",
        "\n",
        "model.invoke(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjJy21NM9yNW",
        "outputId": "637f9361-a616-4673-a8f0-136123822230"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='The language of the user input is German, specifically a dialect from Austria or Bavaria, as \"Griaß di\" is a colloquial greeting meaning \"Hello\" or \"Greetings to you.\"', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 39, 'prompt_tokens': 24, 'total_tokens': 63, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_6fc10e10eb', 'finish_reason': 'stop', 'logprobs': None}, id='run-c4a3986c-a224-4bb8-8781-fc15752c6574-0', usage_metadata={'input_tokens': 24, 'output_tokens': 39, 'total_tokens': 63, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Message objects like the ones listed above convey conversational [roles](https://python.langchain.com/docs/concepts/messages/#role) and hold important data, such as [tool](https://python.langchain.com/docs/concepts/tool_calling/) calls and token usage counts.\n",
        "\n",
        "Instead of receiving the whole output at once, we can stream it as it is generated.\n"
      ],
      "metadata": {
        "id": "FNC5JmTl_buq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Streaming\n",
        "for token in model.stream(messages):\n",
        "    print(token.content, end=\"|\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoPWajJR_dtI",
        "outputId": "95cf3f88-9d00-48a3-beb1-1c560878f775"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|The| language| is| Austrian| German|,| specifically| a| dialect| from| the| region| of| Austria|.| \"|Gr|ia|ß| di|\"| is| a| common| informal| greeting| in| this| dialect|,| equivalent| to| \"|Hello|\"| in| standard| German|.||"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt templates\n",
        "Prompt templates can be used to format messages, by using variables in the text or slotting in a variable list of messages in a certain spot. See https://python.langchain.com/docs/concepts/prompt_templates/\n",
        "\n",
        "#### ChatPromptTemplate\n",
        "ChatPromptTemplates can contain messages of different message roles. We use this to create a template with variables, which can later be invoked with different values for those variables by passing a dictionary which assigns values to those variables(/keys) creating a prompt. This prompt can then be used to invoke the model."
      ],
      "metadata": {
        "id": "LRKu0pNiG-Sg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "\n",
        "system_template = \"Name {number} typical dishes of the country\"\n",
        "\n",
        "\n",
        "# These result in the same ChatPrompTemplate after using .invoke\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
        ")\n",
        "\n",
        "prompt_template_uses_classes = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        SystemMessagePromptTemplate.from_template(system_template),\n",
        "        HumanMessagePromptTemplate.from_template(\"{text}\")\n",
        "    ]\n",
        ")\n"
      ],
      "metadata": {
        "id": "DweylKQ0GvpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ChatPromptValue"
      ],
      "metadata": {
        "id": "PVPG71NtAL4P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = prompt_template.invoke({\"number\": \"two\", \"text\": \"India!\"})\n",
        "\n",
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JvMiAGS6yel",
        "outputId": "478efeac-c3ab-421c-db1d-6627c951d27c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[SystemMessage(content='Name two typical dishes of the country', additional_kwargs={}, response_metadata={}), HumanMessage(content='India!', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = prompt_template_uses_classes.invoke({\"number\": \"two\", \"text\": \"India!\"})\n",
        "# prompt = prompt_template_uses_classes.invoke({\"number\": \"two\", \"text\": \"India!\"})\n",
        "\n",
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYvR7miN8RLW",
        "outputId": "e502beff-0101-4e47-8d29-4124edbccf28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[SystemMessage(content='Name two typical dishes of the country', additional_kwargs={}, response_metadata={}), HumanMessage(content='India!', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " This ChatPromptValue contains two messages, the same object type that we worked with before\n"
      ],
      "metadata": {
        "id": "H4B8w9ZcAdeX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt.to_messages()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xN_YgeYN8dUw",
        "outputId": "bb669f6c-8e0a-4eb7-97b4-0f3afd49b36a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content='Name two typical dishes of the country', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='India!', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(prompt)\n",
        "response = model.invoke(prompt)\n",
        "print(response.content)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zi3ZC9Pc8h5i",
        "outputId": "7bf56880-d54b-43b0-c93b-edb76f910107"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "messages=[SystemMessage(content='Name two typical dishes of the country', additional_kwargs={}, response_metadata={}), HumanMessage(content='India!', additional_kwargs={}, response_metadata={})]\n",
            "Two typical dishes from India are:\n",
            "\n",
            "1. **Biryani** - A flavorful and aromatic rice dish made with basmati rice, meat (such as chicken, mutton, or beef), and a blend of spices. There are various regional variations of biryani, such as Hyderabadi and Lucknowi (Awadhi) biryani.\n",
            "\n",
            "2. **Paneer Tikka** - A popular vegetarian dish made from marinated cubes of paneer (Indian cottage cheese) that are skewered and grilled or roasted. It's often served with mint chutney and is a popular appetizer or snack.\n",
            "\n",
            "These dishes showcase the rich and diverse culinary heritage of India!\n",
            "content=\"Two typical dishes from India are:\\n\\n1. **Biryani** - A flavorful and aromatic rice dish made with basmati rice, meat (such as chicken, mutton, or beef), and a blend of spices. There are various regional variations of biryani, such as Hyderabadi and Lucknowi (Awadhi) biryani.\\n\\n2. **Paneer Tikka** - A popular vegetarian dish made from marinated cubes of paneer (Indian cottage cheese) that are skewered and grilled or roasted. It's often served with mint chutney and is a popular appetizer or snack.\\n\\nThese dishes showcase the rich and diverse culinary heritage of India!\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 135, 'prompt_tokens': 20, 'total_tokens': 155, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_bba3c8e70b', 'finish_reason': 'stop', 'logprobs': None} id='run-13014247-0477-44e1-aeca-c09fd698bc3b-0' usage_metadata={'input_tokens': 20, 'output_tokens': 135, 'total_tokens': 155, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Build a semantic search engine](https://python.langchain.com/docs/tutorials/retrievers/)\n",
        "Search engine over PDF document -retreive passages similar to input query."
      ],
      "metadata": {
        "id": "-bVDcNSr9Kai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "uLKxRg33_GmZ",
        "outputId": "c9d4cae6-0a9e-4df9-ed76-2a2744a3fc0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.13-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.11.10)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting langchain<0.4.0,>=0.3.13 (from langchain-community)\n",
            "  Downloading langchain-0.3.13-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.27 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.3.28)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.3)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.26.4)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.7.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (9.0.0)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.23.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.13->langchain-community) (0.3.3)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.13->langchain-community) (2.10.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain-community) (24.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community) (1.0.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.12.14)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.27->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.13->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.13->langchain-community) (2.27.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (1.2.2)\n",
            "Downloading langchain_community-0.3.13-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain-0.3.13-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.7.0-py3-none-any.whl (29 kB)\n",
            "Downloading marshmallow-3.23.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, pypdf, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain, langchain-community\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.12\n",
            "    Uninstalling langchain-0.3.12:\n",
            "      Successfully uninstalled langchain-0.3.12\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-0.3.13 langchain-community-0.3.13 marshmallow-3.23.2 mypy-extensions-1.0.0 pydantic-settings-2.7.0 pypdf-5.1.0 python-dotenv-1.0.1 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Document class\n",
        "Document class/abstraction: represents unit of text + its metadata"
      ],
      "metadata": {
        "id": "TiSn-PqG_NJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.documents import Document\n",
        "\n",
        "documents = [\n",
        "    Document(\n",
        "        page_content = \"Chana Masala is a classic indian dish\",\n",
        "        metadata= {\"source\": \"alberts-opinion\"},\n",
        "    ),\n",
        "    Document(\n",
        "        page_content = \"Samosa Chat is a great indian street food\",\n",
        "        metadata= {\"source\" : \"alberts-opinion\"}\n",
        "    )\n",
        "]"
      ],
      "metadata": {
        "id": "qy9tqaAT_W1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loading documents"
      ],
      "metadata": {
        "id": "NCiYOhYjAlH3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets load into colab a sample pdf from langchain:\n",
        "!wget -O document.pdf https://raw.githubusercontent.com/langchain-ai/langchain/master/docs/docs/example_data/nke-10k-2023.pdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "XmJL0oeiAtaN",
        "outputId": "85b8526c-a68d-4e4b-d2b7-d82d5906d8a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-20 15:02:08--  https://raw.githubusercontent.com/langchain-ai/langchain/master/docs/docs/example_data/nke-10k-2023.pdf\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2397936 (2.3M) [application/octet-stream]\n",
            "Saving to: ‘document.pdf’\n",
            "\n",
            "document.pdf        100%[===================>]   2.29M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2024-12-20 15:02:08 (27.7 MB/s) - ‘document.pdf’ saved [2397936/2397936]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "[PyPDFLoader](https://python.langchain.com/docs/integrations/document_loaders/pypdfloader/)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qzAoZtOhHsmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"document.pdf\")\n",
        "docs = loader.load()\n",
        "print(f\"PyPDFLoader loaded {len(docs)} documents, defaulting to one document per page\")\n",
        "print(f\"The first 50 chars of documents 10 content are:\\n{docs[10].page_content[:50]}\")\n",
        "print(f\"Document 10 has the following metadata:\\n{docs[10].metadata}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KW6O-sCuBI8G",
        "outputId": "64ab71f0-07bb-4e63-c99c-43781b90b48c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyPDFLoader loaded 107 documents, defaulting to one document per page\n",
            "The first 50 chars of documents 10 content are:\n",
            "Table of Contents\n",
            "INFORMATION ABOUT OUR EXECUTIVE \n",
            "Document 10 has the following metadata:\n",
            "{'source': 'document.pdf', 'page': 10}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Splitting\n",
        "We don't want PyPDFLoaders default splitting of one document per page with no overlap between documents as it is to coarse. We use a **text splitter** to split each document into chunks of around 1000 chars with around 200 chars overlap. (However, this still means that there can not be any overlap between documents/pages. So a sentence starting one one page and ending on another will never be in the same chunk.)\n",
        "\n",
        "[RecursiveCharacterTextSplitter](https://python.langchain.com/docs/how_to/recursive_text_splitter/) takes seperators like commas and newlines into account and recursively splits the document until all chunks are around the specified size.\n",
        "\n"
      ],
      "metadata": {
        "id": "pAmPhF44MHRi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 1000,\n",
        "    chunk_overlap = 200,\n",
        "    add_start_index = True # saves doc character index of first char in chunk\n",
        ")\n",
        "\n",
        "first_document_split = text_splitter.split_documents([docs[0]])\n",
        "fiftieth_document_split = text_splitter.split_documents([docs[50]])\n",
        "all_documents_splits = text_splitter.split_documents(docs)\n",
        "\n",
        "print(f\"Documents 1 and 50 have {len(first_document_split)} and {len(fiftieth_document_split)} chunks. All {len(docs)} docs together have {len(all_documents_splits)} chunks\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-GUUQH4IMai",
        "outputId": "e9a5e89e-5591-41f1-da8c-2b344d2b1d02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documents 1 and 50 have 5 and 4 chunks. All 107 docs together have 516 chunks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Embeddings\n",
        "Embed the document chunks content by using an embedding model, for example by openai\n",
        "\n",
        "See conceptual guide [Embedding models](https://python.langchain.com/docs/concepts/embedding_models/)"
      ],
      "metadata": {
        "id": "nihq1KG5Uoye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
        "\n",
        "embedding_of_first_chunk = embeddings.embed_query(all_documents_splits[0].page_content)\n",
        "embedding_of_second_chunk = embeddings.embed_query(all_documents_splits[1].page_content)\n",
        "\n",
        "print(f\"Embedding size is {len(embedding_of_first_chunk)}\")"
      ],
      "metadata": {
        "id": "_E9269E-Uq_R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cc3a3a7-b2d8-49c2-9e30-2ed1c4650bfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding size is 3072\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Vector Stores\n",
        "We want to store the computed embeddings for all chunks. We can utilize different vector databases like the PostgreSQL extension PGVector or for testing in-memory. See [here](https://python.langchain.com/docs/tutorials/retrievers/#vector-stores)\n",
        "\n",
        "See Conceptual guide [Vector stores](https://python.langchain.com/docs/concepts/vectorstores/) for more in depth knowledge e.g. on advanced search&retrival techniques.\n",
        "\n",
        "adding documents to the vectore store\n"
      ],
      "metadata": {
        "id": "IUvlgLvlnQ3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-core"
      ],
      "metadata": {
        "id": "QQuizvvpuCDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.vectorstores import InMemoryVectorStore\n",
        "vector_store = InMemoryVectorStore(embeddings)\n",
        "\n",
        "first_fifty_chunks = all_documents_splits[:50]\n",
        "ids = vector_store.add_documents(documents= first_fifty_chunks)"
      ],
      "metadata": {
        "id": "QPl5tc39uPDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids[:5]"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UlA6owy0Vpf",
        "outputId": "429e332d-fb2b-44ca-86f2-277c560e512b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['d9131143-0fc9-43b6-93f5-38dad2714d81',\n",
              " '9da7a518-2eff-4eb6-8c9d-5465a1bf3488',\n",
              " 'd328af62-1320-49b9-b676-eb697a906db2',\n",
              " 'a2870561-ed93-4bcb-b559-865d8fa86312',\n",
              " '26b2ee64-b776-4f5e-bec1-3c9118ea9d1e']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Querying the vector store\n"
      ],
      "metadata": {
        "id": "qKtnZoteu8up"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "first_fifty_chunks[30].page_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "mQKRDnvI0kCV",
        "outputId": "7a505e06-6ba0-4be0-bfa8-81f147e74cf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'In recent years, uncertain global and regional economic and political conditions have affected international trade and increased protectionist actions around the\\nworld. These trends are affecting many global manufacturing and service sectors, and the footwear and apparel industries, as a whole, are not immune. Companies in our\\nindustry are facing trade protectionism in many different regions, and, in nearly all cases, we are working together with industry groups to address trade issues and reduce\\nthe impact to the industry, while observing applicable competition laws. Notwithstanding our efforts, protectionist measures have resulted in increases in the cost of our\\nproducts, and additional measures, if implemented, could adversely affect sales and/or profitability for NIKE, as well as the imported footwear and apparel industry as a\\nwhole.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performing a similarity search on all chunks in our vector store"
      ],
      "metadata": {
        "id": "iXVGdhTQ29Ch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = vector_store.similarity_search(\"What problems necessitated working together?\", k=2)"
      ],
      "metadata": {
        "id": "pgImRsyW0-bH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfV2BmNe5gDV",
        "outputId": "61fa8845-02b9-495d-ad52-8b447b545f42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(id='fa8bf488-14ef-4029-a82b-6072a280d703', metadata={'source': 'document.pdf', 'page': 6, 'start_index': 4929}, page_content='restrictions, we work with a broad coalition of global businesses and trade associations representing a wide variety of sectors to help ensure that any legislation enacted\\nand implemented (i) addresses legitimate and core concerns, (ii) is consistent with international trade rules and (iii) reflects and considers domestic economies and the\\nimportant role they may play in the global economic community.\\nWhere trade protection measures are implemented, we believe we have the ability to develop, over a period of time, adequate alternative sources of supply for the\\nproducts obtained from our present suppliers. If events prevented us from acquiring products from our suppliers in a particular country, our operations could be temporarily\\ndisrupted and we could experience an adverse financial impact. However, we believe we could abate any such disruption, and that much of the adverse impact on supply')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "async version"
      ],
      "metadata": {
        "id": "gS7qYZuv5qOq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = await vector_store.asimilarity_search(\"When was Nike incorporated?\")\n",
        "\n",
        "print(results[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErskUD4L5rfD",
        "outputId": "d6f1bc3a-d570-4a08-837a-310be855ddc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='Table of Contents\n",
            "PART I\n",
            "ITEM 1. BUSINESS\n",
            "GENERAL\n",
            "NIKE, Inc. was incorporated in 1967 under the laws of the State of Oregon. As used in this Annual Report on Form 10-K (this \"Annual Report\"), the terms \"we,\" \"us,\" \"our,\"\n",
            "\"NIKE\" and the \"Company\" refer to NIKE, Inc. and its predecessors, subsidiaries and affiliates, collectively, unless the context indicates otherwise.\n",
            "Our principal business activity is the design, development and worldwide marketing and selling of athletic footwear, apparel, equipment, accessories and services. NIKE is\n",
            "the largest seller of athletic footwear and apparel in the world. We sell our products through NIKE Direct operations, which are comprised of both NIKE-owned retail stores\n",
            "and sales through our digital platforms (also referred to as \"NIKE Brand Digital\"), to retail accounts and to a mix of independent distributors, licensees and sales' metadata={'source': 'document.pdf', 'page': 3, 'start_index': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Return scores:"
      ],
      "metadata": {
        "id": "NRQ1pWaz54AF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_scored = vector_store.similarity_search_with_score(\"What problems necessitated working together?\", k=2)"
      ],
      "metadata": {
        "id": "KXfzlTVe53Om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for result in results_scored:\n",
        "  print(f\"The score for document {result[0].id} is {result[1]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPj8WxKT5zlW",
        "outputId": "1dde310a-e030-42db-8133-f377a6d0c837"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The score for document fa8bf488-14ef-4029-a82b-6072a280d703 is 0.2087\n",
            "The score for document 28200f85-05c3-4d73-9ad8-a5e1ca5fd9f3 is 0.2039\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Search embeddings directly"
      ],
      "metadata": {
        "id": "YCKBZPL7CF-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_embedding = embeddings.embed_query(\"Unrelated text to see the score\")\n",
        "\n",
        "results_sample_embedding = vector_store.similarity_search_with_score_by_vector(sample_embedding)\n",
        "for result in results_sample_embedding:\n",
        "  print(f\"The score for document {result[0].id} is {result[1]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "869v9L0JCKeH",
        "outputId": "1044b548-b24d-4045-bfdd-60aaea0459f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The score for document 7fb145a7-a722-4d11-82f5-e26248ce6ead is 0.1876\n",
            "The score for document d328af62-1320-49b9-b676-eb697a906db2 is 0.1760\n",
            "The score for document a2870561-ed93-4bcb-b559-865d8fa86312 is 0.1707\n",
            "The score for document 4ff0ea3b-5427-4c94-9de6-bd2f55a2179b is 0.1676\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results[0][0].page_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "id": "XU3XbnMhC_DF",
        "outputId": "71396143-d8f7-4f3a-a599-4d6037547ccc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Table of Contents\\nNIKE, INC.ANNUAL REPORT ON FORM 10-KTABLE OF CONTENTS\\nPAGE\\nPART I 1\\nITEM 1. Business 1\\nGeneral 1\\nProducts 1\\nSales and Marketing 2\\nOur Markets 2\\nSignificant Customer 3\\nProduct Research, Design and Development 3\\nManufacturing 3\\nInternational Operations and Trade 4\\nCompetition 5\\nTrademarks and Patents 5\\nHuman Capital Resources 6\\nAvailable Information and Websites 7\\nInformation about our Executive Officers 8\\nITEM 1A.Risk Factors 9\\nITEM 1B.Unresolved Staff Comments 24\\nITEM 2. Properties 24\\nITEM 3. Legal Proceedings 24\\nITEM 4. Mine Safety Disclosures 24\\nPART II 25\\nITEM 5. Market for Registrant's Common Equity, Related Stockholder Matters and Issuer Purchases of Equity Securities 25\\nITEM 6. Reserved 27\\nITEM 7. Management's Discussion and Analysis of Financial Condition and Results of Operations 28\\nITEM 7A.Quantitative and Qualitative Disclosures about Market Risk 49\\nITEM 8. Financial Statements and Supplementary Data 51\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Retreivers\n",
        "`VectorStore` objects are not `Runnables`. However we can construct `Retreivers` from `VectorStore`s which subclass `Runnables`.\n",
        "\n",
        "This functionality can be achived without subclassing as well like this"
      ],
      "metadata": {
        "id": "56HS50liDNwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.runnables import chain\n",
        "\n",
        "# \\@chain  is a decorator/decorated function -> retreiver is an object, not a function\n",
        "# an RunnableLambda(retreiver) object implementing the Runnable interface\n",
        "@chain\n",
        "def retreiver(query:str)-> List[Document]:\n",
        "  return vector_store.similarity_search(query, k=1)\n",
        "\n",
        "retreiver.batch(\n",
        "    [\n",
        "        \"What year was this document created in\",\n",
        "        \"Which shoe brand is relevant here\"\n",
        "    ]\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzIB9hVIFZkf",
        "outputId": "8c0288ac-5cb4-4f30-d747-0c85d4a93c8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[Document(id='26646b59-0c65-4aa1-abcf-cce59c6658a4', metadata={'source': 'document.pdf', 'page': 1, 'start_index': 0}, page_content=\"Table of Contents\\nAs of July 12, 2023, the number of shares of the Registrant's Common Stock outstanding were:\\nClass A 304,897,252 \\nClass B 1,225,074,356 \\n1,529,971,608 \\nDOCUMENTS INCORPORATED BY REFERENCE:\\nParts of Registrant's Proxy Statement for the Annual Meeting of Shareholders to be held on September 12, 2023, are incorporated by reference into Part III of this report.\")],\n",
              " [Document(id='a8a25008-8a41-467c-bcd4-ef7cf34137fc', metadata={'source': 'document.pdf', 'page': 7, 'start_index': 757}, page_content='leisure footwear companies, athletic and leisure apparel companies, sports equipment companies and large companies having diversified lines of athletic and leisure\\nfootwear, apparel and equipment, including adidas, Anta, ASICS, Li Ning, lululemon athletica, New Balance, Puma, Under Armour and V.F. Corporation, among others.\\nThe intense competition and the rapid changes in technology and consumer preferences in the markets for athletic and leisure footwear and apparel and athletic\\nequipment constitute significant risk factors in our operations. Refer to Item 1A. Risk Factors for additional information.\\nNIKE is the largest seller of athletic footwear and apparel in the world. Important aspects of competition in this industry are:\\n• Product attributes such as quality; performance and reliability; new product style, design, innovation and development; as well as consumer price/value.')]]"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retreiver"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDXDjakCNduo",
        "outputId": "4fc3e66b-cd49-4986-8d17-418a4be25acf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RunnableLambda(retreiver)"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Langchains `as_retreiver` method of `vectorStore`\n",
        "\n"
      ],
      "metadata": {
        "id": "dd15op8wK7tu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retreiver_from_vectore_store_method = vector_store.as_retreiver(\n",
        "  search_type=\"similarity\",\n",
        "  search_kwargs={\"k\":1},\n",
        ")\n",
        "\n",
        "retreiver_from_vectore_store_method.batch(\n",
        "    [\n",
        "        \"What year was this document created in\",\n",
        "        \"Which shoe brand is relevant here\"\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "t09hoOG_K6HZ",
        "outputId": "39cb5ec6-48ff-4ec7-e992-5a037b02e896"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'InMemoryVectorStore' object has no attribute 'as_retreiver'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-38ea90897229>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m retreiver_from_vectore_store_method = vector_store.as_retreiver(\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0msearch_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"similarity\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0msearch_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"k\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'InMemoryVectorStore' object has no attribute 'as_retreiver'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apparently not possible with `InMemoryVectorStore`\n",
        "\n",
        "**Next steps and learn more about** [Building a semantic search engine](https://python.langchain.com/docs/tutorials/retrievers/#learn-more)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QH1qLSvJPtEc"
      }
    }
  ]
}